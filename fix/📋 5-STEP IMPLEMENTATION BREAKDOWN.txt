ğŸ“‹ 5-STEP IMPLEMENTATION BREAKDOWN (20% Each)
STEP 1: Core Infrastructure (20% - Phase 1)
Duration: Days 1-2 (16 hours)
Deliverables:

âœ… Project structure created
âœ… Configuration module (config.py with pydantic-settings)
âœ… FastAPI application (main.py with routes)
âœ… ADB Tool (adb_tool.py - tap, swipe, input, press_key)
âœ… Screenshot Tool (screenshot_tool.py - capture, stream compression)
âœ… Vision Tool (vision_tool.py - OCR, AI Vision integration)
âœ… Verification Tool (verification_tool.py - screen comparison)
âœ… Unified Toolkit (toolkit.py - singleton wrapper)
âœ… Unit tests for all tools

Files Created: 23 files (backend structure + core tools)
Acceptance: Can execute toolkit.tap(540, 1080) successfully

STEP 2: LangGraph Workflow (20% - Phase 2)
Duration: Days 3-4 (16 hours)
Deliverables:

âœ… State definition (state.py with AgentState TypedDict)
âœ… All 15 node implementations (nodes.py):

detect_mode, rag_retrieval, check_learned
capture_screen, ai_analyze, plan_action
direct_execute, execute_adb, verify_result
save_learned, wait_human, apply_guidance
next_step, log_results, parse_intent


âœ… Conditional routing logic (edges.py)
âœ… Graph compilation (graph.py)
âœ… Agent orchestrator service (agent_orchestrator.py)

Files Created: 5 files (langgraph module)
Acceptance: Workflow executes end-to-end in mock mode (no real device)

STEP 3: RAG System + Services (20% - Phase 3 + Services)
Duration: Day 5 + partial Day 6 (16 hours)
Deliverables:

âœ… RAG Tool implementation (rag_tool.py)
âœ… Dual collection setup (test_cases + learned_solutions)
âœ… Embedding pipeline
âœ… Learned solutions file format
âœ… Test case indexing from Excel
âœ… Save/retrieve learned solutions
âœ… Screen streamer service (screen_streamer.py)
âœ… Verification engine service (verification_engine.py)
âœ… Models/Schemas (enums.py, results.py, schemas.py)

Files Created: 8 files (RAG + services + models)
Acceptance: RAG retrieves test cases and learned solutions correctly

STEP 4: Web Dashboard (20% - Phase 4)
Duration: Days 6-7 (16 hours)
Deliverables:

âœ… Complete HTML (index.html - layout, all UI elements)
âœ… Custom CSS (styles.css - animations, scrollbar styling)
âœ… Complete JavaScript (app.js - all functionality):

WebSocket connections (screen, logs, status)
API call functions
UI update functions
Event handlers
Click-to-tap canvas interaction


âœ… API route implementations (all 11 endpoints)
âœ… WebSocket handlers (all 3 streams)
âœ… Integration with backend services

Files Created: 3 frontend + 5 backend route files
Acceptance: UI functional, live stream working, can start test execution

STEP 5: Integration & Testing (20% - Phase 5)
Duration: Days 8-10 (16 hours)
Deliverables:

âœ… E2E test: Branch A (test execution flow)
âœ… E2E test: Branch B (standalone command flow)
âœ… E2E test: HITL (human-in-the-loop)
âœ… Performance optimization (achieve 15+ FPS, <100ms response)
âœ… Error handling review and improvement
âœ… Documentation updates (README, inline comments, user guide)
âœ… Bug fixing (all critical and high priority)
âœ… Final testing (automated + manual checklist)
âœ… Release preparation

Files Created: 10+ test files, updated docs
Acceptance: All tests pass, performance targets met, demo successful

ğŸš€ STEP 1 BEGINS NOW!
I will create the following WITHOUT a core/ folder:
Phase 1.1: Project Structure (5 minutes)
bashâœ… Create all directories
âœ… Create __init__.py files
âœ… Create .env.example
âœ… Create requirements.txt
âœ… Create .gitignore
Phase 1.2: Configuration (10 minutes)
bashâœ… backend/config.py (with pydantic-settings)
Phase 1.3: Models (15 minutes)
bashâœ… backend/models/enums.py
âœ… backend/models/results.py
âœ… backend/models/schemas.py
Phase 1.4: Tools (60 minutes) - THE CORE WORK
bashâœ… backend/tools/adb_tool.py        (new, clean)
âœ… backend/tools/screenshot_tool.py (new, clean)
âœ… backend/tools/vision_tool.py     (new, clean)
âœ… backend/tools/verification_tool.py (new, clean)
âœ… backend/tools/rag_tool.py        (new, clean)
âœ… backend/tools/toolkit.py         (new, unified interface)
Phase 1.5: FastAPI Setup (20 minutes)
bashâœ… backend/main.py (FastAPI app)
âœ… backend/routes/__init__.py
âœ… backend/routes/test_execution.py (skeleton)
âœ… backend/routes/standalone.py (skeleton)
âœ… backend/routes/hitl.py (skeleton)
âœ… backend/routes/status.py (skeleton)
âœ… backend/routes/stream.py (skeleton)
Phase 1.6: Unit Tests (20 minutes)
bashâœ… tests/unit/test_adb_tool.py
âœ… tests/unit/test_vision_tool.py
âœ… tests/conftest.py (pytest fixtures)

yes proceed with phase 2.1, important note: think deeply and create only the required files and only one short md files i dont want any other content for better maintanance of the chat and token limit for you.


# Step 3: RAG System + Services - Phase Breakdown

## Overview
- **Duration:** 16 hours (Day 5 + partial Day 6)
- **Goal:** Implement RAG system for test cases and learned solutions
- **Total Phases:** 5 phases
- **Files to Create:** 8 files

---

## Phase 3.1: RAG Foundation (3 hours)

### Goals
- Set up ChromaDB with dual collections
- Configure embedding model
- Implement base RAG operations

### Files
1. `backend/tools/rag_tool.py` - Main RAG implementation (~300 lines)
2. `backend/config.py` - Update with RAG settings

### Deliverables
- ChromaDB initialized with 2 collections:
  - `test_cases` - Store test case definitions
  - `learned_solutions` - Store successful executions
- Embedding pipeline working (SentenceTransformers)
- Basic operations: add, get, delete, search

### Acceptance
```python
rag = RAGTool()
rag.initialize()  # Creates collections
assert rag.test_cases_collection is not None
assert rag.learned_solutions_collection is not None
```

---

## Phase 3.2: Test Case Indexing (3 hours)

### Goals
- Parse Excel test case files
- Index test cases to ChromaDB
- Implement search and retrieval

### Files
3. `backend/tools/excel_parser.py` - Excel parsing utility (~150 lines)
4. `data/test_cases/sample_tests.xlsx` - Sample test cases

### Deliverables
- Excel parser reads test cases from `.xlsx` files
- Test case format:
  ```json
  {
    "test_id": "TEST-001",
    "title": "Open Settings",
    "component": "Settings",
    "steps": ["Step 1", "Step 2"],
    "expected": "Settings screen opens"
  }
  ```
- Index test cases to ChromaDB
- Retrieval by test_id or similarity search

### Acceptance
```python
# Index test cases
rag.index_test_cases_from_excel("data/test_cases/sample_tests.xlsx")

# Retrieve by ID
test = rag.get_test_description("TEST-001")
assert test["title"] == "Open Settings"

# Search by similarity
results = rag.search_similar_tests("open settings menu")
assert len(results) > 0
```

---

## Phase 3.3: Learned Solutions (3 hours)

### Goals
- Define learned solution schema
- Implement save/retrieve operations
- Track success rates

### Files
5. `backend/models/learned_solution.py` - Learned solution schema (~100 lines)

### Deliverables
- Learned solution format:
  ```json
  {
    "test_id": "TEST-001",
    "title": "Open Settings",
    "component": "Settings",
    "steps": [
      {
        "step": 1,
        "description": "Tap Settings icon",
        "action": "tap",
        "coordinates": [850, 450],
        "success": true
      }
    ],
    "execution_count": 5,
    "success_count": 4,
    "success_rate": 0.8,
    "last_execution": "2025-12-28T10:00:00",
    "created_at": "2025-12-27T10:00:00"
  }
  ```
- Save learned solutions after successful test runs
- Retrieve learned solutions by test_id
- Update success rate on repeated executions

### Acceptance
```python
# Save learned solution
rag.save_learned_solution(
    test_id="TEST-001",
    title="Open Settings",
    component="Settings",
    steps=[{"step": 1, "action": "tap", "coordinates": [850, 450]}]
)

# Retrieve
solution = rag.get_learned_solution("TEST-001")
assert solution is not None
assert solution["success_rate"] > 0
```

---

## Phase 3.4: Services Layer (4 hours)

### Goals
- Implement screen streamer service
- Enhance verification engine
- Integrate with existing workflow

### Files
6. `backend/services/screen_streamer.py` - Real-time screen streaming (~200 lines)
7. `backend/services/verification_engine.py` - Enhanced verification (~150 lines)

### Deliverables

#### Screen Streamer
- Continuous screenshot capture
- Stream screenshots via WebSocket
- Configurable frame rate (1-5 fps)
- Buffer management

#### Verification Engine
- Compare screenshots (already in verification_tool)
- Element verification (text, image)
- State verification
- Performance metrics

### Acceptance
```python
# Screen streamer
streamer = ScreenStreamer(fps=2)
await streamer.start()
# WebSocket receives screenshots every 0.5s

# Verification engine
engine = VerificationEngine()
result = engine.verify_element_exists("Settings", screenshot)
assert result.verified == True
```

---

## Phase 3.5: Models & Schemas (3 hours)

### Goals
- Update enums for new statuses
- Create result models
- Update API schemas

### Files
8. `backend/models/results.py` - Test result models (~100 lines)
9. Update `backend/models/enums.py` - Add new enums
10. Update `backend/models/schemas.py` - Add RAG-related schemas

### Deliverables

#### New Enums
```python
class TestResult(str, Enum):
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    BLOCKED = "blocked"

class VerificationStatus(str, Enum):
    VERIFIED = "verified"
    NOT_FOUND = "not_found"
    TIMEOUT = "timeout"
```

#### Result Models
```python
class TestExecutionResult(BaseModel):
    test_id: str
    result: TestResult
    duration: float
    steps_executed: int
    steps_passed: int
    errors: List[str]
    screenshots: List[str]
```

#### RAG Schemas
```python
class IndexTestCasesRequest(BaseModel):
    excel_path: str

class SearchTestsRequest(BaseModel):
    query: str
    top_k: int = 5

class LearnedSolutionResponse(BaseModel):
    test_id: str
    success_rate: float
    steps: List[Dict]
```

### Acceptance
```python
# Create result
result = TestExecutionResult(
    test_id="TEST-001",
    result=TestResult.PASSED,
    duration=5.2,
    steps_executed=3,
    steps_passed=3,
    errors=[],
    screenshots=["shot1.png", "shot2.png"]
)
```

---

## Implementation Order

1. **Phase 3.1** (3h) - RAG Foundation
2. **Phase 3.2** (3h) - Test Case Indexing  
3. **Phase 3.3** (3h) - Learned Solutions
4. **Phase 3.4** (4h) - Services Layer
5. **Phase 3.5** (3h) - Models & Schemas

**Total:** 16 hours

---

## Key Dependencies

### External Libraries
```bash
pip install chromadb sentence-transformers openpyxl
```

### Existing Integration Points
- `backend/tools/rag_tool.py` - Already has placeholder methods
- `backend/tools/verification_tool.py` - Enhance with new methods
- `backend/langgraph/nodes.py` - Uses RAG in rag_retrieval node
- `backend/services/agent_orchestrator.py` - Calls RAG methods

---

## Testing Strategy

### Unit Tests (Each Phase)
- `tests/unit/test_rag_tool.py`
- `tests/unit/test_excel_parser.py`
- `tests/unit/test_learned_solutions.py`
- `tests/unit/test_screen_streamer.py`
- `tests/unit/test_verification_engine.py`

### Integration Tests (End of Step 3)
- Full workflow: Index test â†’ Execute â†’ Save learned â†’ Retrieve learned
- RAG + LangGraph integration
- Services integration

---

## Success Criteria

âœ… ChromaDB initialized with 2 collections  
âœ… Can index test cases from Excel  
âœ… Can retrieve test by ID or similarity  
âœ… Can save learned solutions  
âœ… Can retrieve learned solutions with success rate  
âœ… Screen streamer works via WebSocket  
âœ… Verification engine validates elements  
âœ… All models/schemas properly defined  
âœ… Integration with LangGraph workflow  

---

## File Summary (8 Files)

1. `backend/tools/rag_tool.py` - Main RAG (~300 lines)
2. `backend/tools/excel_parser.py` - Excel parsing (~150 lines)
3. `backend/services/screen_streamer.py` - Streaming (~200 lines)
4. `backend/services/verification_engine.py` - Verification (~150 lines)
5. `backend/models/learned_solution.py` - Schema (~100 lines)
6. `backend/models/results.py` - Results (~100 lines)
7. `data/test_cases/sample_tests.xlsx` - Sample data
8. Updates to existing files (config.py, enums.py, schemas.py)

**Total New Code:** ~1,000 lines  
**Ready to start Phase 3.1?** ğŸš€


CLEAN_AI-AGENT/
â”‚
â”œâ”€â”€ ğŸ“ backend/                      # FastAPI Backend
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                      # Entry point (50 lines)
â”‚   â”œâ”€â”€ config.py                    # Configuration (100 lines)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ routes/                   # API Routes (5 files, ~150 lines each)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_execution.py
â”‚   â”‚   â”œâ”€â”€ standalone.py
â”‚   â”‚   â”œâ”€â”€ hitl.py
â”‚   â”‚   â”œâ”€â”€ status.py
â”‚   â”‚   â””â”€â”€ stream.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ services/                 # Business Logic (3 files)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ agent_orchestrator.py    # (200 lines - orchestration only)
â”‚   â”‚   â”œâ”€â”€ screen_streamer.py       # (150 lines)
â”‚   â”‚   â””â”€â”€ verification_engine.py   # (200 lines)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ tools/                    # Agent Tools (6 files)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ toolkit.py               # (100 lines - unified interface)
â”‚   â”‚   â”œâ”€â”€ adb_tool.py              # (200 lines)
â”‚   â”‚   â”œâ”€â”€ vision_tool.py           # (300 lines)
â”‚   â”‚   â”œâ”€â”€ screenshot_tool.py       # (150 lines)
â”‚   â”‚   â”œâ”€â”€ verification_tool.py     # (150 lines)
â”‚   â”‚   â””â”€â”€ rag_tool.py    
â”‚   â”‚   â””â”€â”€ excel_parser.py        
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ langgraph/                # LangGraph Workflow (4 files)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ state.py                 # (100 lines - AgentState)
â”‚   â”‚   â”œâ”€â”€ nodes.py                 # (400 lines - 15 node functions)
â”‚   â”‚   â”œâ”€â”€ edges.py                 # (100 lines - routing logic)
â”‚   â”‚   â””â”€â”€ graph.py                 # (100 lines - graph compilation)
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ models/                   # Data Models (3 files)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ enums.py                 # (100 lines)
â”‚       â”œâ”€â”€ results.py               # (200 lines - data classes)
â”‚       â””â”€â”€ schemas.py               # (300 lines - Pydantic)
â”‚
â”œâ”€â”€ ğŸ“ frontend/                     # Web UI (3 files)
â”‚   â”œâ”€â”€ index.html                   
â”‚   â”œâ”€â”€ styles.css                   
â”‚   â””â”€â”€ app.js                      
â”‚
â”œâ”€â”€ ğŸ“ data/                         # Data Storage
â”‚   â”œâ”€â”€ ğŸ“ test_cases/
â”‚   â”œâ”€â”€ ğŸ“ prompts/
â”‚   â”œâ”€â”€ ğŸ“ vector_db/
â”‚   â”œâ”€â”€ ğŸ“ results/
â”‚   â””â”€â”€ ğŸ“ screenshots/
â”‚              
â”œâ”€â”€ ğŸ“ tests/                        # Test Suite
â”‚   â”œâ”€â”€ ğŸ“ unit/
â”‚   â”œâ”€â”€ ğŸ“ integration/
â”‚   â””â”€â”€ ğŸ“ e2e/
â”‚
â”œâ”€â”€ ğŸ“ scripts/                      # Utility Scripts
â”‚   â”œâ”€â”€ init_rag.py
â”‚   â”œâ”€â”€ index_test_cases.py
â”‚   â””â”€â”€ health_check.py
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


python -m uvicorn backend.main:app --reload